#Improt Modules
from crawl_data.utils.login import FacebookLogin
from crawl_data.scrapers.crawl_comment import CrawlComment


# imprt th∆∞ vi·ªán c·ªßa selenim
from selenium.webdriver.chrome.webdriver import WebDriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException, TimeoutException, WebDriverException

from database.db.crawl_url_repository import CrawlUrlRepository


import pandas as pd
from time import sleep
import random



class CrawlPost:
    def __init__(self, driver: WebDriver, cookies_file: str, word_search: str):
        # Kh·ªüi t·∫°o c√°c thu·ªôc t√≠nh c·∫ßn thi·∫øt
        self.driver = driver
        self.cookies_file = cookies_file
        self.word_search = word_search

        # Xpath c·ªßa ph·∫ßn post 
        self.xpath_post_link = "//a[contains(@href, '/posts/')]"

        # Xpath c·ªßa ph·∫ßn c√†o comment fanpages
        self.xpath_button_comment = "//div[@role='button' and @id and contains(., 'b√¨nh lu·∫≠n')]"
        self.button_close = "//div[@aria-label='ƒê√≥ng'and @role = 'button']"
        self.posts_element = "//div[@aria-posinset and @aria-describedby]"

        self.xpath_search = "//div[@aria-label='T√¨m ki·∫øm' and @role = 'button']"
        self.xpath_menuitem_search = "//div[@role='menuitem']"
        self.xpath_menu = '//div[contains(@aria-label, "Xem th√™m") and @aria-haspopup="menu"]'
        self.input = "/html/body/div[1]/div/div[1]/div/div[4]/div/div/div[1]/div/div[2]/div/div/div/div/div/div/div[1]/div/div/label/input"
        self.repo = CrawlUrlRepository()
    def send_keys_randomly(self, element, text):
        """Nh·∫≠p k√Ω t·ª± v√†o √¥ input v·ªõi ƒë·ªô tr·ªÖ ng·∫´u nhi√™n ƒë·ªÉ tr√°nh b·ªã ph√°t hi·ªán l√† bot."""
        for char in text:
            element.send_keys(char)
            sleep(random.uniform(0.1, 0.3))

    def crawl_comment_groups_by_post(self, quantity: int, list_url_group:list ):
        """ Crawl danh s√°ch post_id t·ª´ group Facebook """

        # Danh s√°ch l∆∞u ID b√†i vi·∫øt
        comments = []
        idx = None

        # L·∫∑p qua t·ª´ng group ƒë·ªÉ l·∫•y b√†i vi·∫øt
        isLogin = FacebookLogin(driver=self.driver,
                            cookie_path=self.cookies_file).login_with_cookies()
        
        for i, url in enumerate(list_url_group):
            id_url = self.repo.get_id_by_url(link=url)
            
            print(id_url)

            stop_crawling = False
            if not isLogin:
                print(f"‚ùå Kh√¥ng th·ªÉ ƒëƒÉng nh·∫≠p fanpage {url}")
                continue

            print("‚úÖ V√†o groups:", i)
            self.driver.get(url + f"search/?q={self.word_search}")

            comment_check = []
            sleep(random.uniform(3, 5))

            for scroll_time in range(10):
                if stop_crawling:
                    break

                print(f"üîÑ Cu·ªôn trang load b√†i vi·∫øt l·∫ßn {scroll_time}")
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                sleep(random.uniform(5, 7))     
                
                try:
                    idx = - 1
                    link_element = WebDriverWait(self.driver, 15).until(
                        EC.presence_of_all_elements_located((By.XPATH, self.xpath_button_comment))
                    )
                    for idx, link in enumerate(link_element):
                        if stop_crawling:
                            break
                        try:
                            # Ki·ªÉm tra link c√≥ c√≤n trong DOM kh√¥ng
                            self.driver.execute_script("return arguments[0].offsetParent !== null;", link)
                        except StaleElementReferenceException:
                            print(f"‚ùå Link {idx} kh√¥ng c√≤n trong DOM (StaleElementReferenceException).")
                            continue
                        except Exception as e:
                            print(f"‚ùå L·ªói khi ki·ªÉm tra link {idx}:")
                            continue

                        self.driver.execute_script("arguments[0].scrollIntoView();", link)
                        self.driver.execute_script("arguments[0].click();", link)
                        print("ƒë√£ click v√†o: ", link.text)
                        sleep(random.uniform(2, 3))
                        print("B·∫Øt ƒë·∫ßu crawl comments")
                        comment_data = CrawlComment(driver=self.driver, cookies_file=self.cookies_file).crawl_comment_group(word_search=self.word_search, isgroup=True, index=id_url)
                                
                        if comment_data:
                            print(f"‚úÖ L·∫•y xong b√†i post th·ª©: {idx}")
                            comments.extend(comment_data)
                            comment_check.append(1)
                            print("Comment check" , len(comment_check))     
                        if len(comment_check) >= quantity:
                            stop_crawling = True
                            break         
                except (NoSuchElementException, TimeoutException, StaleElementReferenceException, WebDriverException) as e:
                    print(f"‚ùå B√†i vi·∫øt {idx} kh√¥ng c√≥ b√¨nh lu·∫≠n ho·∫∑c l·ªói")
                    continue
                except Exception as e:
                    print(f"L·ªói kh√¥ng x√°c ƒë·ªãnh khi x·ª≠ l√Ω b√†i post {idx}")
                    continue
        return pd.DataFrame(comments)



    """H√†m n√†y d√πng ƒë·ªÉ l·∫•y comment d·ª±a tr√™n b√†i post c·ªßa fanpages"""
    def crawl_comment_fanpages_by_post(self, fanpage_urls: list, quantity: int):   
        comments = []
        idx = None

        isLogin = FacebookLogin(driver=self.driver, cookie_path=self.cookies_file).login_with_cookies()
        for i, url in enumerate(fanpage_urls):

            id_url = self.repo.get_id_by_url(link=url)

            stop_crawling = False
            if not isLogin:
                print(f"‚ùå Kh√¥ng th·ªÉ ƒëƒÉng nh·∫≠p fanpage {url}")
                continue

            print("‚úÖ V√†o fanpage:", i)
            self.driver.get(url)
            sleep(random.uniform(3, 5))

            comment_check = []

            try:
                # Th·ª≠ t√¨m ki·∫øm tr·ª±c ti·∫øp
                search_elm = WebDriverWait(self.driver, 2).until(
                    EC.presence_of_element_located((By.XPATH, self.xpath_search))
                )
                self.driver.execute_script("arguments[0].click();", search_elm)
                sleep(random.uniform(1, 2))

            except (NoSuchElementException, TimeoutException, StaleElementReferenceException, WebDriverException):
                try:
                    # Fallback: m·ªü menu v√† ch·ªçn item
                    menu_elm = WebDriverWait(self.driver, 5).until(
                        EC.presence_of_element_located((By.XPATH, self.xpath_menu))
                    )
                    self.driver.execute_script("arguments[0].click();", menu_elm)
                    sleep(random.uniform(1, 2))

                    menu_item_elm = WebDriverWait(self.driver, 5).until(
                        EC.presence_of_element_located((By.XPATH, self.xpath_menuitem_search))
                    )
                    self.driver.execute_script("arguments[0].click();", menu_item_elm)
                    sleep(random.uniform(1, 2))

                except (NoSuchElementException, TimeoutException, StaleElementReferenceException, WebDriverException):
                    pass  # C·∫£ menu c≈©ng fail th√¨ b·ªè qua

            # D√π l√† nh√°nh n√†o, n·∫øu input t·ªìn t·∫°i th√¨ x·ª≠ l√Ω
            try:
                input_elm = WebDriverWait(self.driver, 2).until(
                    EC.presence_of_element_located((By.XPATH, self.input))
                )
                self.send_keys_randomly(input_elm, self.word_search)
                input_elm.send_keys(Keys.ENTER)

            except (NoSuchElementException, TimeoutException, StaleElementReferenceException, WebDriverException):
                pass

            for scroll_time in range(10):
                if stop_crawling:
                    break

                print(f"üîÑ Cu·ªôn trang load b√†i vi·∫øt l·∫ßn {scroll_time}")
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                sleep(random.uniform(5, 7))     
                
                try:
                    idx = - 1
                    link_element = WebDriverWait(self.driver, 5).until(
                        EC.presence_of_all_elements_located((By.XPATH, self.xpath_button_comment))
                    )
                    for idx, link in enumerate(link_element):
                        if stop_crawling:
                            break
                        try:
                            # Ki·ªÉm tra link c√≥ c√≤n trong DOM kh√¥ng
                            self.driver.execute_script("return arguments[0].offsetParent !== null;", link)
                        except StaleElementReferenceException:
                            print(f"‚ùå Link {idx} kh√¥ng c√≤n trong DOM (StaleElementReferenceException).")
                            continue
                        except Exception as e:
                            print(f"‚ùå L·ªói khi ki·ªÉm tra link {idx}")
                            continue

                        self.driver.execute_script("arguments[0].scrollIntoView();", link)
                        self.driver.execute_script("arguments[0].click();", link)
                        print("ƒë√£ click v√†o: ", link.text)
                        sleep(random.uniform(4, 6))
                        print("B·∫Øt ƒë·∫ßu crawl comments")
                        comment_data = CrawlComment(driver=self.driver, cookies_file=self.cookies_file).crawl_comment(word_search=self.word_search, isfanpage=True, index=id_url)
                        
                        if comment_data:
                            print(f"‚úÖ L·∫•y xong b√†i post th·ª©: {idx}")
                            comments.extend(comment_data)
                            comment_check.append(1)
                            print("Comment check" , len(comment_check))     
                        if len(comment_check) >= quantity:
                            stop_crawling = True
                            break
                except (NoSuchElementException, TimeoutException, StaleElementReferenceException, WebDriverException) as e:
                    print(f"‚ùå B√†i vi·∫øt {idx} kh√¥ng c√≥ b√¨nh lu·∫≠n ho·∫∑c l·ªói")
                    continue
                except Exception as e:
                    print(f"L·ªói kh√¥ng x√°c ƒë·ªãnh khi x·ª≠ l√Ω b√†i post {idx}")
                    continue
        return pd.DataFrame(comments)