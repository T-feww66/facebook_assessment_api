#Improt Modules
from crawl_data.utils.login import FacebookLogin
from crawl_data.scrapers.crawl_comment import CrawlComment


# imprt th∆∞ vi·ªán c·ªßa selenim
from selenium.webdriver.chrome.webdriver import WebDriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import NoSuchElementException, StaleElementReferenceException, TimeoutException, WebDriverException

import pandas as pd
from time import sleep
import random



class CrawlPost:
    def __init__(self, driver: WebDriver, cookies_file: str, word_search: str):
        # Kh·ªüi t·∫°o c√°c thu·ªôc t√≠nh c·∫ßn thi·∫øt
        self.driver = driver
        self.cookies_file = cookies_file
        self.word_search = word_search

        # Xpath c·ªßa ph·∫ßn post 
        self.xpath_post_link = "//a[contains(@href, '/posts/')]"
        # self.xpath_post_content = "//div[@data-testid='post_body']"

        # Xpath c·ªßa ph·∫ßn c√†o comment fanpages
        self.xpath_button_comment = "//div[@role='button' and @id and contains(., 'b√¨nh lu·∫≠n')]"
        self.button_close = "//div[@aria-label='ƒê√≥ng'and @role = 'button']"
        self.posts_element = "//div[@aria-posinset and @aria-describedby]"

    # def clean_data(self, df):
    #     df = df.drop_duplicates(subset="post_id")
    #     return df
    
    def crawl_comment_groups_by_post(self, group_file: str, quantity: int ):
        """ Crawl danh s√°ch post_id t·ª´ group Facebook """
        # ƒê·ªçc danh s√°ch group_id t·ª´ file CSV
        df = pd.read_csv(group_file)
        group_urls = df["group_url"].tolist()

        # Danh s√°ch l∆∞u ID b√†i vi·∫øt
        comments = []
        stop_crawling = False

        # L·∫∑p qua t·ª´ng group ƒë·ªÉ l·∫•y b√†i vi·∫øt
        isLogin = FacebookLogin(driver=self.driver,
                            cookie_path=self.cookies_file).login_with_cookies()
        
        for i, url in enumerate(group_urls):
            if not isLogin:
                print(f"‚ùå Kh√¥ng th·ªÉ ƒëƒÉng nh·∫≠p fanpage {url}")
                continue

            print("‚úÖ V√†o groups:", i)
            self.driver.get(url + f"search/?q={self.word_search}")

            comment_check = []
            sleep(random.uniform(3, 5))

            for scroll_time in range(10):
                if stop_crawling:
                    break

                print(f"üîÑ Cu·ªôn trang load b√†i vi·∫øt l·∫ßn {scroll_time}")
                sleep(random.uniform(5, 7))     
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                sleep(random.uniform(5, 7))     
                
                try:
                    link_element = WebDriverWait(self.driver, 15).until(
                        EC.presence_of_all_elements_located((By.XPATH, self.xpath_button_comment))
                    )
                    for idx, link in enumerate(link_element):
                        if stop_crawling:
                            break
                        try:
                            # Ki·ªÉm tra link c√≥ c√≤n trong DOM kh√¥ng
                            self.driver.execute_script("return arguments[0].offsetParent !== null;", link)
                        except StaleElementReferenceException:
                            print(f"‚ùå Link {idx} kh√¥ng c√≤n trong DOM (StaleElementReferenceException).")
                            continue
                        except Exception as e:
                            print(f"‚ùå L·ªói khi ki·ªÉm tra link {idx}: {e}")
                            continue

                        self.driver.execute_script("arguments[0].scrollIntoView();", link)
                        self.driver.execute_script("arguments[0].click();", link)
                        print("ƒë√£ click v√†o: ", link.text)
                        sleep(random.uniform(4, 6))
                        print("B·∫Øt ƒë·∫ßu crawl comments")
                        comment_data = CrawlComment(driver=self.driver, cookies_file=self.cookies_file).crawl_comment(brand_name=self.word_search, isgroup=True)
                        
                        if comment_data:
                            print(f"‚úÖ L·∫•y xong b√†i post th·ª©: {idx}")
                            comments.extend(comment_data)
                            comment_check.append(1)
                            print("Comment check" , len(comment_check))     
                        if len(comment_check) >= quantity:
                            stop_crawling = True
                            break         
                except (NoSuchElementException, TimeoutException, StaleElementReferenceException, WebDriverException) as e:
                    print(f"‚ùå B√†i vi·∫øt {idx} kh√¥ng c√≥ b√¨nh lu·∫≠n ho·∫∑c l·ªói", e)
                    continue
                except Exception as e:
                    print(f"L·ªói kh√¥ng x√°c ƒë·ªãnh khi x·ª≠ l√Ω b√†i post {idx}")
                    continue
        return pd.DataFrame(comments)


    """H√†m n√†y d√πng ƒë·ªÉ l·∫•y comment d·ª±a tr√™n b√†i post c·ªßa fanpages"""
    def crawl_comment_fanpages_by_post(self, fanpages_file: str, quantity: int):
        
        df = pd.read_csv(fanpages_file)
        fanpage_urls = df["fanpage_url"].tolist()
        comments = []
        comment_check = []
        stop_crawling = False

        for i, url in enumerate(fanpage_urls):
            isLogin = FacebookLogin(driver=self.driver, cookie_path=self.cookies_file).login_with_cookies()
            if not isLogin:
                print(f"‚ùå Kh√¥ng th·ªÉ ƒëƒÉng nh·∫≠p fanpage {url}")
                continue

            print("‚úÖ V√†o fanpage:", i)
            self.driver.get(url)
            sleep(random.uniform(3, 5))

            for scroll_time in range(20):
                if stop_crawling:
                    break

                print(f"üîÑ Cu·ªôn trang load b√†i vi·∫øt l·∫ßn {scroll_time}")
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                
                sleep(random.uniform(5, 10))     
                try:
                    link_element = WebDriverWait(self.driver, 15).until(
                        EC.presence_of_all_elements_located((By.XPATH, self.xpath_button_comment))
                    )
                    for idx, link in enumerate(link_element):
                        if stop_crawling:
                            break
                        try:
                            # Ki·ªÉm tra link c√≥ c√≤n trong DOM kh√¥ng
                            self.driver.execute_script("return arguments[0].offsetParent !== null;", link)
                        except StaleElementReferenceException:
                            print(f"‚ùå Link {idx} kh√¥ng c√≤n trong DOM (StaleElementReferenceException).")
                            continue
                        except Exception as e:
                            print(f"‚ùå L·ªói khi ki·ªÉm tra link {idx}: {e}")
                            continue

                        self.driver.execute_script("arguments[0].scrollIntoView();", link)
                        self.driver.execute_script("arguments[0].click();", link)
                        print("ƒë√£ click v√†o: ", link.text)
                        sleep(random.uniform(4, 6))
                        print("B·∫Øt ƒë·∫ßu crawl comments")
                        comment_data = CrawlComment(driver=self.driver, cookies_file=self.cookies_file).crawl_comment(brand_name=self.word_search, isfanpage=True)
                        
                        if comment_data:
                            print(f"‚úÖ L·∫•y xong b√†i post th·ª©: {idx}")
                            comments.extend(comment_data)
                            comment_check.append(1)
                            print("Comment check" , len(comment_check))     
                        if len(comment_check) >= quantity:
                            stop_crawling = True
                            break         
                except (NoSuchElementException, TimeoutException, StaleElementReferenceException, WebDriverException) as e:
                    print(f"‚ùå B√†i vi·∫øt {idx} kh√¥ng c√≥ b√¨nh lu·∫≠n ho·∫∑c l·ªói", e)
                    continue
                except Exception as e:
                    print(f"L·ªói kh√¥ng x√°c ƒë·ªãnh khi x·ª≠ l√Ω b√†i post {idx}")
                    continue
        return pd.DataFrame(comments)
    def run(self):
        """ Ch·∫°y t·∫•t c·∫£ c√°c qu√° tr√¨nh """
        self.crawl_post_id()
        self.driver.quit()